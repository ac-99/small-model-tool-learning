# README: _Tool Learning for SLMs_

This repo contains 3 primary modules: `data_cleaning`, `baseline`, `training`. 


#### Disclaimer

**WARNING: the repo is not immediately reproducible** -- this work was performed on Google Colaboratory. While the `data_cleaning/download_data.ipynb` will work, after installing the appropriate packages, the `main.py` and other packages will not immediately run. These will need to be imported into Google Colaboratory and pointed to the cleaned data generated by data_cleaning. Due to resource constraints (lack of access to a local GPU), it was not possible to recreate a working version of the code. However, the steps to do so are outlined here.

1. `data_cleaning/download_data.ipynb` -> `data_cleaning/download_data.py` so that it can be easily called by `main.py`
2. `baseline` -- both notebooks using OpenAI models could be converted into a single `.py` with a different constant for `MODEL`. The Phi-3 script could be converted separately. Several functions would be shared between the two scripts, so these could be placed into a third script, `baseline/baseline_utils.py`. Results would also need to be stored in `main.py`
3. `training` -- these training runs could be represented by a single script which was parameterised with three different `LR` values.
4. `main.py` would need to orchestrate all of the above and log results. Currently, results are logged using `wandb`; these results could be pulled and output in the required format desribed in the current `main.py` template.

If run locally, the evaluation scripts would all be in their own folder `evals` so that there was no repeated code.

Finally, the code is documented in a way to reduce repetition. See the `README.md` in each folder for details.

#### Module description

A description of the modules are below; more details can be found in the report. Any modules which are non-standard in Colaboratory are included as `!pip install [package]` in the `.ipynb` files:

1. `data_cleaning/`

Data cleaning contains a single file, `download_data.ipynb`. This script downloads the data from Hugging Face Datasets using the Hugging Face `datasets` library. This is then loaded from `.json` files into a `pandas.DataFrame` -- excluding one file in test-data (see report for details).

2. `baseline/`

The baseline module contains scripts to generate completions on the cleaned data and run evaluations. The outputs are available in `outputs/`. Phi-3 baseline is run using Hugging Face `transformers` and requires a GPU; the OpenAI models make API calls. Evaluation is then run identically for all approaches.

3. `training/`

Training contains 3 identical scripts with one different parameter, learning rate. The details of why are described in the report. These scripts load the Phi-3 (or any other) model using `transformers` using a `bitsandbytes` config (for qLoRA), and then runs the fine tuning using a mix of `peft`, `trl`'s `SFTTrainer` and `transformers`' `TrainingArguments`. The same evaluation scripts are run at the end. Training and results use `wandb`. 


#### Packages used

Refer to the in the `requirements.txt`.